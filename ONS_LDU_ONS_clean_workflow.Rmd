---
title: "ONS / Postal Code Crosswalk: Workflow and Results"
subtitle: "Normalized to OSM Residential Areas"
author: "Christopher Belanger"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggspatial)
library(osmdata)
library(sf)
library(sp)
library(leaflet)
library(tictoc)
# load ONS Gen2 shapefile
url <- "https://opendata.arcgis.com/datasets/32fe76b71c5e424fab19fec1f180ec18_0.geojson"
ons_shp <- sf::read_sf(url) %>%
  sf::st_make_valid() 
```

# Summary

This RMarkdown document shows the code and methodology for creating an area crosswalk between postal codes and the residential areas of ONS neighbourhoods. The intention is twofold:

1. By using residential area instead of total land area, we hope to distribute populations more accurately between neighbourhoods.
1. By normalizing our intersection percentages to intersecting area, rather than total area, we will ensure that all Ottawa residents are counted completely. (In other words, if a postal code is only partially inside Ottawa, any Ottawa resident in that postal code will be completely attributed to one or more ONS neighbourhoods.)

The process is as follows:

1. **First,** I took our ONS neighbourhood boundaries and trimmed them to only include residential zones according to OpenStreepMaps (OSM) on October 29, 2020. I inspected the resulting polygons visually, and found them accurate and without any obvious errors.
1. **Second,** for each postal code, I found its intersection with each trimmed ONS neighbourhood and calculated the area of each intersection.
1. **Third,** I converted each postal code's intersection area to a percentage by dividing by that postal code's total intersecting area. In other words, we normalize to the postal code's residential area, not to its total area.

I prepared both long and short datafiles and saved them as .csv.

**Note** This version was updated on December 1, 2020, to include all postal codes in the adjusted area-weighted files. Postal codes that do not intersect any residential areas are weighted based on their land areas.

# Collecting the OSM Data

The OSM data was collected using the Overpass API through [overpass turbo](https://overpass-turbo.eu/). Here is the API call:


```
/*
This has been generated by the overpass-turbo wizard.
The original search was:
“landuse=residential”
*/
[out:json][timeout:10000];
// gather results
(
  // query part for: “landuse=residential”
  way["landuse"="residential"]({{bbox}});
  relation["landuse"="residential"]({{bbox}});
);
// print results
out body;
>;
out skel qt;
```

Results were then loaded and combined into a single multipolygon for processing:

```{r osm_union, eval=FALSE}
tic()
osm_res <- read_sf("shapefiles/osm_residential_2020.10.29.geojson") %>%
  sf::st_union()
toc()

osm_res %>%
  write_sf("shapefiles/osm_residential_union_2020.10.29.geojson")


```

Here is a plot of the residential zones in the Ottawa region according to OSM:

```{r load_osm_union}
osm_res <- read_sf("shapefiles/osm_residential_union_2020.10.29.geojson")

osm_res %>%
  ggplot() +
  geom_sf()
```

# Standardize CRSs

I ran the analysis using two different coordinate reference systems: WGS84, and [NAD83/MTM zone 9](https://georepository.com/crs_32189/NAD83-MTM-zone-9.html). The difference in final results was negligible (one intersection changed by 0.6%, four changed by 0.1%, and no other changed out of ~2.2 million rows). I've used the projected NAD83/MTM CRS in this code.

```{r reproject}

# project everything to epsg 32189 
ons_shp <- sf::st_transform(ons_shp, 32189)
osm_res <- sf::st_transform(osm_res, 32189)
```



# Extract the residential regions from the ON shapefile

Now we trim the ONS neighbourhoods of all non-residential zones, by taking each ONS polygon's intersection with the OSM-identified residential zones. This should take under 10 seconds.

```{r do_trim, warning=F, message=F}
library(tictoc)

ons_trim <- sf::st_intersection(ons_shp, osm_res)


ons_trim <- ons_trim %>%
  select(-Shape__Area, -Shape__Length)
```

When we plot the results, they look right.

```{r plot_trim}

ons_trim %>%
  sf::st_transform(crs="WGS84") %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons(label =ons_trim$Name)

```

# Find intersections with LDUs

Now we load the LDU (postal code) shapefile, and transform it to the same CRS as the other files.

```{r load_ldus}

ldus <- read_sf("../1 - New/shapefiles/LDU_union.shp") %>%
   sf::st_transform(32189)

```


Then, for memory reasons we break the big set of LDUs into batches for processing. We take 1000 postal codes at a time, find their intersections with each ONS neighbourhood, then find area of each intersection, and then find the percentage the total intersecting area in each intersection. In other words, for each postal code we find 

*This takes 35-45 minutes on my machine.*

```{r main_algorithm, eval = FALSE}


# get the list of postal codes
pcodes <- ldus %>%
  pull(POSTALCODE) %>%
  sort() %>%
  unique()


# set batch size: how many postal codes will we process at once?
batch_size <- 1000
#batch_size <- 1
# how many batches will we need? it seems ok to give  a vector an index that's too big, it returns NA
num_batches <- (length(pcodes) / batch_size ) %>% ceiling()
#num_batches <- 1

# here is our results tibble.
results <- tibble()

# eventually we will iterate over batches
for (batch in 1:num_batches) {
  #batch <- 1
  message(batch,"/",num_batches)
  # now we filer the big shape to get the postal codes we want
  # and make sure the ldus are valid
  ldus_batch <- ldus %>%
    filter(POSTALCODE %in% pcodes[((batch-1)*batch_size + 1):(batch*batch_size)]) %>%
    st_make_valid()
  
  if (batch_size == 1) message(batch,"/",num_batches, ": ", ldus_batch$POSTALCODE)
  
  tic()
  ldus_intersect <- ldus_batch %>%
    #mutate(geometry = purrr::map(geometry, st_buffer, dist = 0)) %>%
    mutate(results = purrr::map(geometry, function(x) {
      purrr::map(ons_trim$geometry, st_intersection, y=x) %>%
        purrr::map_dbl(st_area) %>%
        tibble(ONS_ID = ons_trim$ONS_ID, Name = ons_trim$Name, intersection_area = .)
    })) %>%
    st_set_geometry(NULL)
  toc()
  
  results <- bind_rows(results, ldus_intersect)
} #end for


# this is where we do the intersection calculations
# for each postal code, we find the total area that intersects with any neighbourhood
# then we find the percentage of that value that each intersection represents
# so it is NOT % of total postal code area; it is % of postal code area that intersects any residential area.
# so the 
ldus_unnest <- results %>%
  unnest(cols = results) %>%
  group_by(POSTALCODE) %>%
  mutate(total_intersection_area = sum(intersection_area),
         intersection_pct = intersection_area / total_intersection_area,
         intersection_pct = round(intersection_pct, digits=3 )) %>%
  filter(intersection_pct > 0 ) %>%
  select(POSTALCODE, ONS_ID, intersection_pct) %>%
  pivot_wider(names_from = ONS_ID, values_from = intersection_pct) %>%
  select(POSTALCODE, sort(current_vars()))

ldus_unnest %>%
  write_csv("ldus_intersect.csv")
```


Then we can save our results as both long and wide files:

```{r save_results, eval=FALSE}

ldus_unnest %>%
 #head(n=1) %>%
  mutate(across(where(is.numeric), function(x) if_else(is.na(x), 0, x))) %>%
  write_csv("LDUs_ONS_wide.csv")

ldus_long <- ldus_unnest %>%
  pivot_longer(cols = -POSTALCODE, names_to = "ONS_ID", values_to = "pct_intersect") %>%
  mutate(pct_intersect = if_else(is.na(pct_intersect), 0, pct_intersect))

ldus_long %>%
  filter(pct_intersect>0) %>% 
  write_csv("LDUs_ONS_long.csv")

```


# Comparing to the WGS84 calculations

The difference is minimal: in 5 cases we have a difference of, at most, 0.6%, and in the other ~2.2 million cases we have no difference at all.

```{r load_wgs, message=F, warning=F}
ldus_long <- read_csv("results/LDUs_ONS_long.csv")

ldus_long_wgs <- read_csv("results/LDUs_ONS_long-wgs84.csv") %>%
  rename(pct_wgs = pct_intersect)

ldus_compare <- ldus_long %>%
  mutate(ONS_ID = as.double(ONS_ID)) %>%
  left_join(ldus_long_wgs, by = c("POSTALCODE","ONS_ID")) %>%
  rename (pct_nad = pct_intersect) %>%
    mutate(pct_wgs= if_else(is.na(pct_wgs), 0, pct_wgs),
           diff = pct_nad - pct_wgs)

ldus_compare %>%
  arrange(desc(diff)) %>%
  head(10)
```


# Creating Single-Link Indicator File

Here we're creating a Single-Link Indicator (SLI) that assigns each postal code to one and only one ONS neighbourhood. We take the long list of postal codes and the neighbourhoods they intersect, and then for each postal code take the neighbourhood with the biggest intersection percentage.

```{r make_sli}

  ldus_sli <- ldus_long %>%
    group_by(POSTALCODE) %>%
    filter(pct_intersect == max(pct_intersect)) %>%
    select(-pct_intersect)

ldus_sli %>%
  write_csv("results/LDUs_ONS_SLI.csv")

```

However, this only covers postal codes that currently intersect residential zones. To future-proof this list, we should also include postal codes that don't currently intersect residential areas.

To do so, we'll use the results from an earlier run (similar methodology, different Rmd file) that allocated postal codes based *only* on area. For postal codes that are not in our current SLI, we will take the values from this previous run.


```{r}

# Load the long LDU/ONS crosswalk based on normal area overlap
ldus_long_area <- read_csv("../1 - New/LDUs_ONS_area_long.csv") %>%
  filter(pct_intersect>0)

# filter out any postal codes that appear in the weighted-area SLI, and assign remaining postal codes to one neighbourhood
ldus_area_sli <- ldus_long_area %>%
  filter(!(POSTALCODE %in% ldus_sli$POSTALCODE)) %>%
  group_by(POSTALCODE) %>%
    filter(pct_intersect == max(pct_intersect)) %>%
    select(-pct_intersect)

# combine the weighted-area and area-based SLI lists into one dataframe
ldus_all_sli <- ldus_area_sli %>%
  bind_rows(ldus_sli)

# the result has 21,973 rows, which is the same as the # of postalcodes that intersect an ONS neighbourhood (I found this in another analysis)

ldus_all_sli %>%
  arrange(POSTALCODE) %>%
  write_csv("results/LDUs_all_ONS_SLI.csv")
```   

# Augmenting the Weighted Files

The long and wide weightings we created only include postal codes that currently cover residential zones. As we did just above for the SLI, we can also include postal codes that don't currently intersect residential areas. 


We'll repeat the process we used for the SLI and use the results from an earlier run (similar methodology, different Rmd file) that allocated postal codes based *only* on area. For postal codes that are not in our current weighted tables, we will take the values from this previous run.

```{r}
ldus_long <- read_csv("../3 - Greenspace/results/LDUs_ONS_long.csv")

# Load the long LDU/ONS crosswalk based on normal area overlap
ldus_long_area <- read_csv("../1 - New/LDUs_ONS_area_long.csv") %>%
  filter(pct_intersect>0)

# filter out any postal codes that appear in the weighted-area SLI, and assign remaining postal codes to one neighbourhood
ldus_long_missing <- ldus_long_area %>%
  filter(!(POSTALCODE %in% ldus_long$POSTALCODE)) %>%
  arrange(POSTALCODE) %>%
  group_by(POSTALCODE) %>%
  mutate(total_intersect = sum(pct_intersect),
         pct_of_total = pct_intersect/total_intersect) %>%
  select(-pct_intersect, -total_intersect, pct_intersect = pct_of_total)


# combine the weighted-area and area-based SLI lists into one dataframe
ldus_all_long <- ldus_long_missing %>%
  bind_rows(ldus_long) %>%
  arrange(POSTALCODE)

# the result has 21,973 rows, which is the same as the # of postalcodes that intersect an ONS neighbourhood (I found this in another analysis)

ldus_all_long %>%
  write_csv("results/LDUs_all_ONS_long.csv")

```

We can create an augmented "wide" version as well:

```{r}
  
ldus_all_long %>%
    pivot_wider(names_from = ONS_ID, 
              values_from = pct_intersect,
              values_fill = 0) %>%
  select(everything()) %>%
  write_csv("results/LDUs_all_ONS_wide.csv")
```



# Visualizing some test cases

Looking at my neighbourhood, this looks correct:

```{r test1}
ldus %>%
  filter(POSTALCODE == "K1H7S5") %>%
  st_transform(crs = "WGS84") %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons(color = "red") %>%
  addPolygons(data = ons_trim %>% 
                filter(str_detect(Name, "Alta Vista")) %>% 
                         st_transform(crs = "WGS84"), 
              label="Alta Vista")
```

And we see the results look correct in our intersection table:

```{r}
ldus_long %>%
  filter(POSTALCODE == "K1H7S5")
```

Looking at K1C1T1, it has a more complex relationship with three neighbourhoods:

```{r test2}

for_plotting <- ons_trim %>% 
                filter(str_detect(Name, "Chapel Hill") | str_detect(Name, "Chateau") | str_detect(Name, "Queenswood")) %>%
  st_transform(crs = "WGS84")

ldus %>%
  filter(POSTALCODE == "K1C1T1") %>%
  st_transform(crs = "WGS84") %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons(color = "red", label = "K1C1T1") %>%
  addPolygons(data = for_plotting, 
              label=for_plotting$Name)


```

And again, eyeballing the areas, this looks correct.

```{r}
ldus_long %>%
  filter(POSTALCODE=="K1C1T1")
```

